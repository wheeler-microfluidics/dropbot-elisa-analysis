from collections import namedtuple
from copy import deepcopy
import cPickle as pickle
import cStringIO as StringIO
import datetime as dt
import re
import time

from microdrop.experiment_log import ExperimentLog
from microdrop.protocol import Protocol
from path_helpers import path
import arrow
import dstat_interface as di
import dstat_interface.analysis
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd


ExperimentLogDir = namedtuple('ExperimentLogDir', ['log_dir', 'instrument_id'])


def combine_data_from_microdrop_logs(exp_log_paths):
    '''
    Scrape microdrop experiment log directories to generate a
    `pandas.DataFrame` with the combined experimental data from a
    set of experiments.

    Args
    ----

        exp_log_paths (list of ExperimentLogDir named tuples) : Each tuple in
        the list describes the path to a Microdrop experiment log directory and
        and instrument_id (optional). If the instrument_id is None, this
        function will try to extract an instrument_id from the experiment log
        metadata.

    Returns
    -------

        (pandas.DataFrame) : Combined experimental data.
    '''
    combined_data_df = pd.DataFrame()

    for log_dir, instrument_id in exp_log_paths:
        exp_id = str(log_dir.name)
        output_path = log_dir / path('wheelerlab.dropbot_dx_accessories')

        # Check for existing csv file which provides a cached version
        # of the data generated by this script.
        cached_file_exists = False

        if output_path.isdir():
            for file_path in output_path.files('*.csv'):
                # skip the calibrator file
                if file_path.name == 'calibrator.csv':
                    continue

                df = pd.DataFrame().from_csv(path=file_path, index_col=None)
                if 'experiment_uuid' in df.columns and 'utc_timestamp' in df.columns:
                    print '%s is a cached data file with the right columns' % file_path
                    df = df.set_index(['experiment_uuid', 'utc_timestamp'])
                    combined_data_df = combined_data_df.append(df)
                    cached_file_exists = True

        # if we have a cached file
        if cached_file_exists:
            continue

        # if there is no dstat data for this experiment, continue
        if not len(log_dir.files('*Measure*.txt')):
            continue

        log_file = log_dir / 'data'

        try:
            # load the experiment log
            print 'load experiment log %s' % log_file
            log = ExperimentLog.load(log_file)
        except Exception, e:
            print "Couldn't load exp %s" % exp_id
            continue

        step_numbers = log.get('step')
        protocol = Protocol.load(log_dir / 'protocol')

        step_labels = []
        for step in protocol.steps:
            step_labels.append(pickle.loads(step.plugin_data['wheelerlab.step_label_plugin'])['label'])

        relative_humidity = []
        temperature_celsius = []

        for line in log.get('environment state', 'wheelerlab.dropbot_dx_accessories'):
            if line:
                temperature_celsius.append(line['temperature_celsius'])
                relative_humidity.append(line['relative_humidity'])
            else:
                temperature_celsius.append(None)
                relative_humidity.append(None)

        dstat_enabled = []
        magnet_engaged = []

        for step in protocol.steps:
            dx_data = pickle.loads(step.plugin_data['wheelerlab.dropbot_dx_accessories'])
            dstat_enabled.append(dx_data['dstat_enabled'])
            magnet_engaged.append(dx_data['magnet_engaged'])

        for file_path in log_dir.files('*Measure*.txt'):
            df = di.analysis.dstat_to_frame(file_path)
            df['experiment_uuid'] = log.uuid
            df['experiment_id'] = log.experiment_id
            df = df.reset_index().set_index(['utc_timestamp', 'experiment_uuid'])

            print file_path.name
            match = re.match('(?P<label>.*)(?P<attempt>\d+)\-data.txt', file_path.name)
            attempt = 0
            if match:
                attempt = int(match.group('attempt'))
            else:
                match = re.match(r'(?P<label>.*)-data.txt', file_path.name)

            label = match.group('label')
            step_number = step_labels.index(label)

            index = 0
            for i in range(attempt + 1):
                index = step_numbers.index(step_number, index + 1)

            try:
                metadata = deepcopy(log.metadata['wheelerlab.metadata_plugin'])
                device_id = metadata.get('device_id', '')
                sample_id = metadata.get('sample_id', '')

                cre_device_id = re.compile(r'#(?P<batch_id>[a-fA-F0-9]+)'
                                           r'%(?P<device_id>[a-fA-F0-9]+)$')

                # If `device_id` is in the form '#<batch-id>%<device-id>', extract batch and
                # device identifiers separately.
                match = cre_device_id.match(device_id)
                if match:
                    metadata['device_id'] = unicode(match.group('device_id'))
                    metadata[u'batch_id'] = unicode(match.group('batch_id'))
                else:
                    metadata['device_id'] = ''
                    metadata[u'batch_id'] = ''

                df['device_id'] = metadata['device_id']
                df['batch_id'] = metadata['batch_id']
                df['sample_id'] = metadata['sample_id']
            except:
                df['device_id'] = ''
                df['batch_id'] = ''
                df['sample_id'] = ''

            # TODO: get instrument_id from experiment log metadata
            df['instrument_id'] = instrument_id

            df['step_number'] = step_number
            df['attempt_number'] = attempt
            df['temperature_celsius'] = temperature_celsius[index]
            df['relative_humidity'] = relative_humidity[index]

            start_time = log.get('start time')[0]
            df['experiment_start'] = dt.datetime.fromtimestamp(start_time).isoformat()
            df['experiment_length_min'] = log.get('time')[-1] / 60

            if not output_path.isdir():
                output_path.mkdir()

            df.to_csv(output_path /
                      path('e[%s]-d[%s]-s[%s].csv' % (log.uuid, df['device_id'].values[0],
                                                      df['sample_id'].values[0])))

            combined_data_df = combined_data_df.append(df)
    return combined_data_df


def reduce_microdrop_dstat_data(df_md_dstat, settling_period_s=2., bandwidth=1.):
    '''
    Reduce measurements for each Microdrop DStat acquisition step in
    `df_md_dstat` to a single row with an aggregate signal value.

    For continuous detection, the aggregate signal column corresponds to the
    mean `current_amps`.

    For synchronous detection experiments (i.e., where `target_hz` is greater
    than 0), the aggregate signal corresponds to the integrated amplitude of
    the `current_amps` FFT within the bandwidth around target frequency.

    See `dstat_interface.analysis.reduce_microdrop_dstat_data` for more
    details.

    Args
    ----

        df_md_dstat (pandas.DataFrame) : Microdrop DStat measurements in a
            table with at least the columns `experiment_uuid`, `step_number`,
            `attempt_number`, `target_hz`, `sample_frequency_hz`, `current_amps`,
            and `time_s`.
        settling_period_s (float) : Measurement settling period in seconds.
            Measurements taken before start time will not be included in
            calculations.
        bandwidth (float) : Bandwidth (centered at synchronous detection
            frequency) to integrate within.

    Returns
    -------

        (pd.DataFrame) : Table containing the columns `experiment_start`,
            `experiment_length_min`, `sample_id`, `experiment_uuid`,
            `step_label`, `instrument_name`, `relative_humidity`,
            `temperature_celsius`, `sample_frequency_hz`, `target_hz`,
            and `signal` (i.e., the aggregate signal value).
    '''
    summary_fields = ['experiment_start', 'experiment_length_min', 'sample_id',
                      'instrument_name', 'relative_humidity', 'temperature_celsius',
                      'sample_frequency_hz', 'target_hz']
    groupby = ['experiment_uuid', 'step_label', 'step_number', 'attempt_number']

    return di.analysis.reduce_dstat_data(df_md_dstat, groupby=groupby,
                                         summary_fields=summary_fields)
